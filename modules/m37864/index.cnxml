<document xmlns="http://cnx.rice.edu/cnxml">

<title>Constructed response items</title>
<metadata xmlns:md="http://cnx.rice.edu/mdml">
  <md:content-id>m37864</md:content-id>
  <md:title>Constructed response items</md:title>
  <md:abstract>A module about constructed response items, such as short answer and extended response questions. This includes a discussion of methods for scoring these types of response items with scoring rubrics and performance assessments.</md:abstract>
  <md:uuid>21ab306c-e190-40c4-821b-6e394ead8b4a</md:uuid>
</metadata>

<content>
    
      <note id="eip-528">The primary author of this module is Dr. Rosemary Sutton.</note><para id="id2036496">Formal assessment also includes constructed response items in which students are asked to recall information and create an answer—not just recognize if the answer is correct—so guessing is reduced. Constructed response items can be used to assess a wide variety of kinds of knowledge and two major kinds are discussed: <emphasis effect="italics">completion or short answer </emphasis>(also called short response) and <emphasis effect="italics">extended response.</emphasis></para>
      <section id="id1164142426496"><title>Completion and short answer </title><para id="id1164137484112">Completion and short answer items can be answered in a word, phrase, number, or symbol. These types of items are essentially the same only varying in whether the problem is presented as a statement or a question (Linn &amp; Miller 2005). For example: </para>
        <para id="id4015112">Completion: The first traffic light in the US was invented by…………….</para>
        <para id="id1164152356792">Short Answer: Who invented the first traffic light in the US?</para>
        <para id="id1164155279473">These items are often used in mathematics tests, e.g.</para>
        <para id="id1164137334211">3 + 10 = …………..? </para>
        <para id="id1164139678672">Draw the line of symmetry on the following shape </para>
<figure id="eip-id1169755909637"><media id="eip-id1169759128254" alt="A large D"><image src="../../media/bigd.png" mime-type="image/png"/></media>
</figure>
        <para id="id1164139794501">A major advantage of these items is they that they are easy to construct. However, apart from their use in mathematics they are unsuitable for measuring complex learning outcomes and are often difficult to score. Completion and short answer tests are sometimes called objective tests as the intent is that there is only one correct answer and so there is no variability in scoring but unless the question is phrased very carefully, there are frequently a variety of correct answers. For example, consider the item</para>
        <para id="id1164141160009">Where was President Lincoln born?....................</para>
        <para id="id1164138406753">The teacher may expect the answer “in a log cabin” but other correct answers are also “on Sinking Spring Farm”, “in Hardin County” or “in Kentucky”. Common errors in these items are summarized in <link target-id="id7577393"/>.</para>
        
        <table id="id7577393" summary="Examples of poorly created constructed response items, organized by format and category.">
<tgroup cols="3"><colspec colnum="1" colname="c1"/>
            <colspec colnum="2" colname="c2"/>
            <colspec colnum="3" colname="c3"/>
            <tbody>
              <row>
                <entry>
                  <emphasis effect="bold">Type of item </emphasis>
                </entry>
                <entry>
                  <emphasis effect="bold">Common errors</emphasis>
                </entry>
                <entry>
                  <emphasis effect="bold">Example</emphasis>
                </entry>
              </row>
              <row>
                <entry>Completion and short answer</entry>
                <entry>There is more than one possible answer.</entry>
                <entry>e.g. Where was US President Lincoln born?<emphasis effect="italics"> The answer could be in a log cabin, in Kentucky etc. </emphasis></entry>
              </row>
              <row>
<entry/>
                <entry>Too many blanks are in the completion item so it is too difficult or doesn’t make sense. </entry>
                <entry>e.g. In ….. theory, the first stage, ….. . is when infants process through their ……. and ….. ……… </entry>
              </row>
              <row>
<entry/>
                <entry>Clues are given by length of blanks in completion items. </entry>
                <entry>e.g. Three states are contiguous to New Hampshire: . ….is to the West, ……is to the East and ………..…. is to the South.</entry>
              </row>
              <row>
                <entry>Extended Response </entry>
                <entry>Ambiguous questions</entry>
                <entry>e.g. Was the US Civil War avoidable? <emphasis effect="italics">Students could interpret this question in a wide variety of ways, perhaps even stating “yes”</emphasis> or<emphasis effect="italics"> “no”. One student may discuss only political causes another moral, political and economic causes. There is no guidance in the question for students.</emphasis></entry>
              </row>
              <row>
<entry/>
                <entry>Poor reliability in grading</entry>
                <entry>The teacher does not use a scoring rubric and so is inconsistent in how he scores answers especially unexpected responses, irrelevant information, and grammatical errors.</entry>
              </row>
              <row>
<entry/>
                <entry>Perception of student influences grading </entry>
                <entry>By spring semester the teacher has developed expectations of each student’s performance and this influences the grading (numbers can be used instead of names).The test consists of three constructed responses and the teacher grades the three answers on each students’ paper before moving to the next paper. This means that the grading of questions 2 and 3 are influenced by the answers to question 1 (teachers should grade all the 1<sup>st</sup> question then the 2<sup>nd</sup> etc).</entry>
              </row>
              <row>
<entry/>
                <entry>Choices are given on the test and some answers are easier than others.</entry>
                <entry>Testing experts recommend not giving choices in tests because then students are not really taking the same test creating equity problems. </entry>
              </row>
            </tbody>
          
</tgroup><caption>Common errors in constructed response items</caption>
</table>
      </section>
      <section id="id1164138324594"><title>Extended response </title><para id="id1164136081882">Extended response items are used in many content areas and answers may vary in length from a paragraph to several pages. Questions that require longer responses are often called <emphasis effect="italics">essay </emphasis>questions. Extended response items have several advantages and the most important is their adaptability for measuring complex learning outcomes— particularly integration and application. These items also require that students write and therefore provide teachers a way to assess writing skills. A commonly cited advantage to these items is their ease in construction; however, carefully worded items that are related to learning outcomes and assess complex learning are hard to devise (Linn &amp; Miller, 2005). Well-constructed items phrase the question so the task of the student is clear. Often this involves providing hints or planning notes. In the first example below the actual question is clear not only because of the wording but because of the format (i.e. it is placed in a box). In the second and third examples planning notes are provided: </para>
        <example id="oo-id1164137274657"><title>Third grade mathematics: </title><para id="id1164155281277">The owner of a bookstore gave 14 books to the school. The principal will give an equal number of books to each of three classrooms and the remaining books to the school library. How many books could the principal give to each student and the school?</para>
          <para id="id1164140253508">Show all your work on the space below and on the next page. Explain in words how you found the answer. Tell why you took the steps you did to solve the problem. </para>
          <para id="id1164137189215">From Illinois Standards Achievement Test, 2006; (<emphasis effect="underline">http://www.isbe.state.il.us/assessment/isat.htm</emphasis>)</para>
         </example>
        <example id="eip-768"><title>Fifth grade science: The grass is always greener</title><para id="id1164140410034">Jose and Maria noticed three different types of soil, black soil, sand, and clay, were found in their neighborhood. They decided to investigate the question, “How does the type of soil (black soil, sand, and clay) under grass sod affect the height of grass?”</para>
          <para id="id1164139698684">Plan an investigation that could answer their new question.</para>
          <para id="id1164146158483">In your plan, be sure to include:</para>
<list id="eip-id1167294319766">
          <item id="id1164139636662">Prediction of the outcome of the investigation</item>
          <item id="id1164138225592">Materials needed to do the investigation</item>
          <item id="id1164139020636">Procedure that includes:
<list id="eip-id1167301790229">
          <item id="id1164142113102">logical steps to do the investigation</item>

          <item id="id3538784">one variable kept the same (controlled)</item>
          <item id="id1164140532998">one variable changed (manipulated)</item>
          <item id="id1164138227738">any variables being measure and recorded</item>
          <item id="id1164140378268">how often measurements are taken and recorded</item></list>
</item>
</list>
          <para id="id1164137791010">(From Washington State 2004 assessment of student learning )</para>
          <para id="id5385684">http://www.k12.wa.us/assessment/WASL/default.aspx)</para>
         
        </example><example id="eip-60"><title>Grades 9-11 English:</title><para id="id1164149159073">Writing prompt</para>
          <para id="id1164149682563">Some people think that schools should teach students how to cook. Other people think that cooking is something that ought to be taught in the home. What do you think? Explain why you think as you do.</para>
          <para id="id1164149439003">Planning notes</para>
          <para id="id1164140712189">Choose One:</para>
          <para id="id1164140292921">□ I think schools should teach students how to cook</para>
          <para id="id1164140933900">□ I think cooking should l be taught in the home</para>
          <para id="id1164136664094">I think cooking should be taught in ……………………………..because……… </para>
          <para id="id1164140481698">(school) or (the home)</para>
          <para id="id1164138516723">(From Illinois Measure of Annual Growth in English <emphasis effect="underline">http://www.isbe.state.il.us/assessment/image.htm</emphasis>)</para></example><para id="id1164140361332">A major disadvantage of extended response items is the difficulty in reliable scoring. Not only do various teachers score the same response differently but also the same teacher may score the identical response differently on various occasions (Linn &amp; Miller 2005). A variety of steps can be taken to improve the reliability and validity of scoring. First, teachers should begin by writing an outline of a model answer. This helps make it clear what students are expected to include. Second, a sample of the answers should be read. This assists in determining what the students can do and if there are any common misconceptions arising from the question. Third, teachers have to decide what to do about irrelevant information that is included (e.g. is it ignored or are students penalized) and how to evaluate mechanical errors such as grammar and spelling. Then, a <emphasis effect="italics">point scoring</emphasis> or<emphasis effect="italics"> a scoring rubric </emphasis>should be used.<emphasis effect="italics"/></para>
        <para id="id1164154435786">In point scoring components of the answer are assigned points. For example, if students were asked:</para>
        <para id="id1164140816921">What are the nature, symptoms, and risk factors of hyperthermia? </para>
        <para id="id1164155944324">Point Scoring Guide:</para>
<list id="eip-id1164467182982" list-type="bulleted" bullet-style="none"><item id="id3826584">Definition (natures)     2 pts</item>
        <item id="id1164154967478">Symptoms (1 pt for each)    5 pts</item>
        <item id="id1164142361539">Risk Factors (1 point for each)  5 pts</item>
        <item id="id1164142238574">Writing      3 pts </item>
</list>
        <para id="id1164135972848">This provides some guidance for evaluation and helps consistency but point scoring systems often lead the teacher to focus on facts (e.g. naming risk factors) rather than higher level thinking that may undermine the validity of the assessment if the teachers’ purposes include higher level thinking. A better approach is to use a scoring rubric that <emphasis effect="italics">describes </emphasis>the quality of the answer or performance at each level.</para>
      </section>
      <section id="id1164140522558"><title>Scoring rubrics</title><para id="id1164137727670">Scoring rubrics can be <emphasis effect="italics">holistic</emphasis> or <emphasis effect="italics">analytical.</emphasis> In holistic scoring rubrics, general descriptions of performance are made and a single overall score is obtained. An example from grade 2 language arts in Los Angeles Unified School District classifies responses into four levels: not proficient, partially proficient, proficient and advanced is on Table 5.</para>
        

<example id="eip-id1168409873260"><title>Example of holistic scoring rubric: English language arts grade 2</title><para id="id1164140685112">Assignment. Write about an interesting, fun, or exciting story you have read in class this year. Some of the things you could write about are:</para>
<list id="eip-id1168401220597">
        <item id="id1164137867930">What happened in the story (the plot or events)</item>
        <item id="id1164142285867">Where the events took place (the setting)</item>
        <item id="id1164142492176">People, animals, or things in the story ( the characters)</item>
</list>
        <para id="id1164138069222">In your writing make sure you use facts and details from the story to describe everything clearly.</para>
        <para id="id1164139596043">After you write about the story, explain what makes the story interesting, fun or exciting.</para>
        <para id="id1164138033341">
          <emphasis effect="bold">Scoring rubric</emphasis>
        </para>
        <table id="id1164138078386" summary="An english language arts scoring rubric example.">
<tgroup cols="2"><colspec colnum="1" colname="c1"/>
            <colspec colnum="2" colname="c2"/>
            <tbody>
              <row>
                <entry>Advanced<newline/> Score 4 </entry>
                <entry>The response demonstrates well-developed reading comprehension skills.Major story elements (plot, setting, or characters) are clearly and accurately described.Statements about the plot, setting, or characters are arranged in a manner that makes sense.Ideas or judgments (why the story is interesting, fun, or exciting) are clearly supported or explained with facts and details from the story.</entry>
              </row>
              <row>
                <entry>Proficient<newline/> Score 3</entry>
                <entry>The response demonstrates solid reading comprehension skills.Most statements about the plot, setting, or characters are clearly described.Most statements about the plot, setting, or characters are arranged in a manner that makes sense.Ideas or judgments are supported with facts and details from the story.</entry>
              </row>
              <row>
                <entry>Partially Proficient<newline/> Score 1</entry>
                <entry>The response demonstrates some reading comprehension skillsThere is an attempt to describe the plot, setting, or characters Some statements about the plot, setting, or characters are arranged in a manner that makes sense.Ideas or judgments may be supported with some facts and details from the story.</entry>
              </row>
              <row>
                <entry>Not Proficient<newline/> Score 1 </entry>
                <entry>The response demonstrates little or no skill in reading comprehension.The plot, setting, or characters are not described, or the description is unclear. Statements about the plot, setting, or characters are not arranged in a manner that makes sense.Ideas or judgments are not stated, and facts and details from the text are not used.</entry>
              </row>
            </tbody>
          

</tgroup>
</table>
        <para id="id1164136019934"><emphasis effect="italics"> Source</emphasis>: Adapted from English Language Arts Grade 2 Los Angeles Unified School District, 2001 (http://www.cse.ucla.edu/resources/justforteachers_set.htm)</para>
</example>
        <para id="id1164137179065">Analytical rubrics provide descriptions of levels of student performance on a variety of characteristics. For example, six characteristics used for assessing writing developed by the Northwest Regional Education Laboratory (NWREL) are: </para>
<list id="eip-id1168406549762">
        <item id="id1164138028585">ideas and content </item>
        <item id="id1164136026430">organization </item>
        <item id="id1164140601211">voice </item>
        <item id="id1164154175093">word choice </item>
        <item id="id1164154553593">sentence fluency </item>
        <item id="id1164155525741">conventions</item>
</list>
        <para id="id1164148133830">Descriptions of high, medium, and low responses for each characteristic are available from: </para>
        <para id="id1164145182100"><emphasis effect="underline">http://www.nwrel.org/assessment/toolkit98/traits/index.html</emphasis>).</para>
        <para id="id1164137796578">Holistic rubrics have the advantages that they can be developed more quickly than analytical rubrics. They are also faster to use as there is only one dimension to examine. However, they do not provide students feedback about which aspects of the response are strong and which aspects need improvement (Linn &amp; Miller, 2005). This means they are less useful for assessment <emphasis effect="italics">for</emphasis> learning. An important use of rubrics is to use them as teaching tools and provide them to students <emphasis effect="italics">before</emphasis> the assessment so they know what knowledge and skills are expected. </para>
        <para id="id1164137640606">Teachers can use scoring rubrics as part of instruction by giving students the rubric during instruction, providing several responses, and analyzing these responses in terms of the rubric. For example, use of accurate terminology is one dimension of the science rubric in Table 6. An elementary science teacher could discuss why it is important for scientists to use accurate terminology, give examples of inaccurate and accurate terminology, provide that component of the scoring rubric to students, distribute some examples of student responses (maybe from former students), and then discuss how these responses would be classified according to the rubric. This strategy of assessment for learning should be more effective if the teacher (a) emphasizes to students why using accurate terminology is important when learning science rather than how to get a good grade on the test (we provide more details about this in the section on motivation later in this chapter); (b) provides an exemplary response so students can see a model; and (c) emphasizes that the goal is student improvement on this skill not ranking students. </para>
        
        <para id="id1164139919189">* On the High School Assessment, the application of a concept to a practical problem or real-world situation will be scored when it is required in the response and requested in the item stem.</para>
        <table id="id1164145304521" summary=""><title>Example of a scoring rubric, Science</title><tgroup cols="6">
            <colspec colnum="1" colname="c1"/>
            <colspec colnum="2" colname="c2"/>
            <colspec colnum="3" colname="c3"/>
            <colspec colnum="4" colname="c4"/>
            <colspec colnum="5" colname="c5"/>
            <colspec colnum="6" colname="c6"/>
            <tbody>
              <row>
                <entry/>
                <entry>
                  <emphasis effect="bold">Level of understanding</emphasis>
                </entry>
                <entry>
                  <emphasis effect="bold">Use of accurate scientific terminology</emphasis>
                </entry>
                <entry>
                  <emphasis effect="bold">Use of supporting details</emphasis>
                </entry>
                <entry>
                  <emphasis effect="bold">Synthesis of information</emphasis>
                </entry>
                <entry>
                  <emphasis effect="bold">Application of information*</emphasis>
                </entry>
              </row>
              <row>
                <entry>4</entry>
                <entry>There is evidence in the response that the student has a full and complete understanding.</entry>
                <entry>The use of accurate scientific terminology enhances the response.</entry>
                <entry>Pertinent and complete supporting details demonstrate an integration of ideas.</entry>
                <entry>The response reflects a complete synthesis of information.</entry>
                <entry>An effective application of the concept to a practical problem or real-world situation reveals an insight into scientific principles.</entry>
              </row>
              <row>
                <entry>3</entry>
                <entry>There is evidence in the response that the student has a good understanding.</entry>
                <entry>The use of accurate scientific terminology strengthens the response.</entry>
                <entry>The supporting details are generally complete.</entry>
                <entry>The response reflects some synthesis of information.</entry>
                <entry>The concept has been applied to a practical problem or real-world situation.</entry>
              </row>
              <row>
                <entry>2</entry>
                <entry>There is evidence in the response that the student has a basic understanding.</entry>
                <entry>The use of accurate scientific terminology may be present in the response.</entry>
                <entry>The supporting details are adequate.</entry>
                <entry>The response provides little or no synthesis of information.</entry>
                <entry>The application of the concept to a practical problem or real-world situation is inadequate.</entry>
              </row>
              <row>
                <entry>1</entry>
                <entry>There is evidence in the response that the student has some understanding.</entry>
                <entry>The use of accurate scientific terminology is not present in the response.</entry>
                <entry>The supporting details are only minimally effective.</entry>
                <entry>The response addresses the question.</entry>
                <entry>The application, if attempted, is irrelevant.</entry>
              </row>
              <row>
                <entry>0</entry>
                <entry namest="c2" nameend="c6">The student has <emphasis effect="bold">NO UNDERSTANDING</emphasis> of the question or problem. The response is completely incorrect or irrelevant.</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </section>
      <section id="id1164140266052"><title>Performance assessments</title><para id="id1164138548210">Typically in performance assessments students complete a specific task while teachers observe the process or procedure (e.g. data collection in an experiment) as well as the product (e.g. completed report) (Popham, 2005; Stiggens, 2005). The tasks that students complete in performance assessments are not simple—in contrast to selected response items—and include the following:</para>
<list id="eip-id1166329459547">
        <item id="id1164147181323">playing a musical instrument</item>
        <item id="id1164139019476">athletic skills</item>
        <item id="id1164137009018">artistic creation </item>
        <item id="id1164135957289">conversing in a foreign language</item>
        <item id="id1164134698199">engaging in a debate about political issues </item>
        <item id="id1164147179779">conducting an experiment in science </item>
        <item id="id1164141566800">repairing a machine </item>
        <item id="id1164138326392">writing a term paper </item>
        <item id="id1164138559272">using interaction skills to play together</item>
</list>
        <para id="id1164140308697">These examples all involve complex skills but illustrate that the term performance assessment is used in a variety of ways. For example, the teacher may not observe all of the process (e.g. she sees a draft paper but the final product is written during out-of-school hours) and essay tests are typically classified as performance assessments (Airasian, 2000). In addition, in some performance assessments there may be no clear product (e.g. the performance may be group interaction skills). </para>
        <para id="id1164147183455">Two related terms, <emphasis effect="italics">alternative assessment </emphasis>and<emphasis effect="italics"> authentic assessment</emphasis> are sometimes used instead of performance assessment but they have different meanings (Linn &amp; Miller, 2005). Alternative assessment refers to tasks that are not pencil-and-paper and while many performance assessments are not pencil-and paper tasks some are (e.g. writing a term paper, essay tests). Authentic assessment is used to describe tasks that students do that are similar to those in the “real world”. Classroom tasks vary in level of authenticity (Popham, 2005). For example, a Japanese language class taught in a high school in Chicago conversing in Japanese in Tokyo is highly authentic—but only possible in a study abroad program or trip to Japan. Conversing in Japanese with native Japanese speakers in Chicago is also highly authentic, and conversing with the teacher in Japanese during class is moderately authentic. Much less authentic is a matching test on English and Japanese words. In a language arts class, writing a letter (to an editor) or a memo to the principal is highly authentic as letters and memos are common work products. However, writing a five-paragraph paper is not as authentic as such papers are not used in the world of work. However, a five paragraph paper is a complex task and would typically be classified as a performance assessment. </para>
      </section>
      <section id="id1164152771160"><title>Advantages and disadvantages</title><para id="id1164139967069">There are several advantages of performance assessments (Linn &amp; Miller 2005). First, the focus is on complex learning outcomes that often cannot be measured by other methods. Second, performance assessments typically assess process or procedure as well as the product. For example, the teacher can observe if the students are repairing the machine using the appropriate tools and procedures as well as whether the machine functions properly after the repairs. Third, well designed performance assessments communicate the instructional goals and meaningful learning clearly to students. For example, if the topic in a fifth grade art class is one-point perspective the performance assessment could be drawing a city scene that illustrates one point perspective. (<emphasis effect="underline">http://www.sanford-artedventures.com</emphasis>). This assessment is meaningful and clearly communicates the learning goal. This performance assessment is a good instructional activity and has good content validity—common with well designed performance assessments (Linn &amp; Miller 2005). </para>
        <para id="id1164136067150">One major disadvantage with performance assessments is that they are typically very time consuming for students and teachers. This means that fewer assessments can be gathered so if they are not carefully devised fewer learning goals will be assessed—which can reduce content validity. State curriculum guidelines can be helpful in determining what should be included in a performance assessment. For example, Eric, a dance teacher in a high school in Tennessee learns that the state standards indicate that dance students at the highest level should be able to do demonstrate consistency and clarity in performing technical skills by:</para>
<list id="eip-id1170992095284">
        <item id="id1164141589404">performing complex movement combinations to music in a variety of meters and styles</item>
        <item id="id1164136405844">performing combinations and variations in a broad dynamic range</item>
        <item id="id1164138034269">demonstrating improvement in performing movement combinations through self-evaluation </item>
        <item id="id1164136700670">critiquing a live or taped dance production based on given criteria</item>
</list>
        <para id="id1164137916956">(<emphasis effect="underline">http://www.tennessee.gov/education/ci/standards/music/dance912.shtml</emphasis>)</para>
        <para id="id1164142323307">Eric devises the following performance task for his eleventh grade modern dance class<emphasis effect="italics">. </emphasis></para>
        <para id="id1164156447741">
          <emphasis effect="italics">In groups of 4-6 students will perform a dance at least 5 minutes in length. The dance selected should be multifaceted so that all the dancers can demonstrate technical skills, complex movements, and a dynamic range (Items 1-2). Students will videotape their rehearsals and document how they improved through self evaluation (Item 3). Each group will view and critique the final performance of one other group in class (Item 4). Eric would need to scaffold most steps in this performance assessment. The groups probably would need guidance in selecting a dance that allowed all the dancers to demonstrate the appropriate skills; critiquing their own performances constructively; working effectively as a team, and applying criteria to evaluate a dance.</emphasis>
        </para>
        <para id="id1164137938637">Another disadvantage of performance assessments is they are hard to assess reliably which can lead to inaccuracy and unfair evaluation. As with any constructed response assessment, scoring rubrics are very important. An example of holistic and analytic scoring rubrics designed to assess a completed <emphasis effect="italics">product</emphasis> are in Table 5 and Table 6. A rubric designed to assess the<emphasis effect="italics"> process</emphasis> of group interactions is in Table 7. </para>
        
        <table id="id1164138700814" summary="A grading rubric for group interaction."><title>Example of group interaction rubric</title>
<tgroup cols="4"><colspec colnum="1" colname="c1"/>
            <colspec colnum="2" colname="c2"/>
            <colspec colnum="3" colname="c3"/>
            <colspec colnum="4" colname="c4"/>
            <tbody>
              <row>
                <entry>
                  <emphasis effect="bold">Score </emphasis>
                </entry>
                <entry>
                  <emphasis effect="bold">Time management</emphasis>
                </entry>
                <entry>
                  <emphasis effect="bold">Participation and performance in roles </emphasis>
                </entry>
                <entry>
                  <emphasis effect="bold">Shared involvement </emphasis>
                </entry>
              </row>
              <row>
                <entry>0</entry>
                <entry>Group did not stay on task and so task was not completed. </entry>
                <entry>Group did not assign or share roles.</entry>
                <entry>Single individual did the task.</entry>
              </row>
              <row>
                <entry>1</entry>
                <entry>Group was off-task the majority of the time but task was completed.</entry>
                <entry>Groups assigned roles but members did not use these roles. </entry>
                <entry>Group totally disregarded comments and ideas from some members. </entry>
              </row>
              <row>
                <entry>2</entry>
                <entry>Group stayed on task most of the time.</entry>
                <entry>Groups accepted and used some but not all roles.</entry>
                <entry>Group accepted some ideas but did not give others adequate consideration.</entry>
              </row>
              <row>
                <entry>3</entry>
                <entry>Group stayed on task throughout the activity and managed time well. </entry>
                <entry>Group accepted and used roles and actively participated. </entry>
                <entry>Groups gave equal consideration to all ideas.</entry>
              </row>
              <row>
                <entry>4</entry>
                <entry>Group defined their own approach in a way that more effectively managed the activity.</entry>
                <entry>Group defined and used roles not mentioned to them. Role changes took place that maximized individuals’ expertise.</entry>
                <entry>Groups made specific efforts to involve all group members including the reticent members.</entry>
              </row>
            </tbody>
          

</tgroup>
</table>
        <para id="id1164137735674"><emphasis effect="italics">Source</emphasis>: Adapted from Group Interaction ( GI) SETUP ( 2003). Issues, Evidence and You. Ronkonkomo, NY Lab-Aids. (<emphasis effect="underline">http://cse.edc.org/products/assessment/middleschool/scorerub.asp))</emphasis></para>
        <para id="id1164147684723">This rubric was devised for middle grade science but could be used in other subject areas when assessing group process. In some performance assessments several scoring rubrics should be used. In the dance performance example above Eric should have scoring rubrics for the performance skills, the improvement based on self evaluation, the team work, and the critique of the other group. Obviously, devising a good performance assessment is complex and Linn and Miller (2005) recommend that teachers should: </para>
        <para id="id1164138708238">Create performance assessments that require students to use complex cognitive skills. Sometimes teachers devise assessments that are interesting and that the students enjoy but do not require students to use higher level cognitive skills that lead to significant learning. Focusing on high level skills and learning outcomes is particularly important because performance assessments are typically so time consuming.</para>
        <para id="id1164141307118">Ensure that the task is clear to the students. Performance assessments typically require multiple steps so students need to have the necessary prerequisite skills and knowledge as well as clear directions. Careful scaffolding is important for successful performance assessments. </para>
        <para id="id5811845">Specify expectations of the performance clearly by providing students scoring rubrics during the instruction. This not only helps students understand what it expected but it also guarantees that teachers are clear about what they expect. Thinking this through while planning the performance assessment can be difficult for teachers but is crucial as it typically leads to revisions of the actual assessment and directions provided to students. </para>
        <para id="id1164141362255">Reduce the importance of unessential skills in completing the task. What skills are essential depends on the purpose of the task. For example, for a science report, is the use of publishing software essential? If the purpose of the assessment is for students to demonstrate the process of the scientific method including writing a report, then the format of the report may not be significant. However, if the purpose includes integrating two subject areas, science and technology, then the use of publishing software is important. Because performance assessments take time it is tempting to include multiple skills without carefully considering if all the skills are essential to the learning goals. </para>
      </section>
   
</content>

</document>